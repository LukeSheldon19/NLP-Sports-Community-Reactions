{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "add8ac3e-555e-4461-a95a-7fd198c2af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e8aead-9b27-4246-afc3-21713072fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_2024_season_schedule = pd.read_csv('C:/Users/LukeB/Downloads/NBA-Full-Schedule-frills-2023-2024.csv')\n",
    "\n",
    "filtered_schedule = nba_2024_season_schedule[\n",
    "    (nba_2024_season_schedule['home_team'].isin(['NYK', 'MEM'])) |\n",
    "    (nba_2024_season_schedule['away_team'].isin(['NYK', 'MEM']))\n",
    "]\n",
    "\n",
    "filtered_schedule = filtered_schedule.reset_index(drop=True)\n",
    "\n",
    "filtered_schedule.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75e9906-aaf9-4c23-8a50-d2e3e7922b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_schedule['game_date'] = pd.to_datetime(filtered_schedule['game_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "492032bf-42d5-4fa9-a5bd-607f5eae14f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import re\n",
    "\n",
    "CLIENT_ID = \"XFQo87DDjus70pAmVFCi-g\"\n",
    "CLIENT_SECRET = \"gZnURWimF9GKa191vjS0jaA4FYaG4w\"\n",
    "USER_AGENT = \"SubstantialAd2572\"\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id = CLIENT_ID,\n",
    "    client_secret = CLIENT_SECRET,\n",
    "    user_agent = USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde32353-2848-47df-9703-88228add75d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_Memphis = []\n",
    "for thread in reddit.subreddit('memphisgrizzlies').new(limit=1000):\n",
    "    url = thread.url\n",
    "    if re.search(r'www.reddit.com/r/' , url) and '/comments/' in url:\n",
    "        urls_Memphis.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f75da-8233-424c-8f4e-955bbd8797f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_NY = []\n",
    "for thread in reddit.subreddit('NYKnicks').new(limit=1000):\n",
    "    url = thread.url\n",
    "    if re.search(r'www.reddit.com/r/' , url) and '/comments/' in url:\n",
    "        urls_NY.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980f5a57-4bb8-4a4f-884a-24b945198c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_all = urls_Memphis + urls_NY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd5e08c-6aa7-48e0-a0e4-a3812beaad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "created_times = []\n",
    "post_urls = []\n",
    "m = 0\n",
    "\n",
    "for url in urls_all:\n",
    "    submission = reddit.submission(url=url)\n",
    "    created_times.append(submission.created_utc)\n",
    "    post_urls.append(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "date_link_df = pd.DataFrame({'URL': post_urls}, index=pd.to_datetime(created_times, unit='s'))\n",
    "\n",
    "date_link_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a4809-2718-4abe-9526-19c3a4fc14ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_link_df.to_csv('C:/Users/LukeB/OneDrive/Documents/Datasets/links_data_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c885cbf6-cf13-4384-8090-2da32dbbc55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df = pd.read_csv('C:/Users/LukeB/OneDrive/Documents/Datasets/links_data_full.csv')\n",
    "urls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7258fe61-c1f8-4097-a69a-d529816e8820",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df = urls_df.rename(columns={'Unnamed: 0': 'date'})\n",
    "urls_df = urls_df.sort_values(by='date')\n",
    "urls_df = urls_df.drop(200)\n",
    "urls_df.reset_index(drop=True, inplace=True)\n",
    "urls_df['date'] = pd.to_datetime(urls_df['date'])\n",
    "urls_df['date'] = urls_df['date'].dt.date\n",
    "urls_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c131095a-2a11-4faa-9e5f-0978305ebe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_schedule_fin = filtered_schedule.loc[:, ['game_date', 'away_team', 'home_team']]\n",
    "filtered_schedule_fin.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af8641-54f8-4d18-84e3-fe989efcb398",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_schedule_fin['game_date'] = pd.to_datetime(filtered_schedule_fin['game_date'])\n",
    "\n",
    "urls_df['date'] = pd.to_datetime(urls_df['date'])\n",
    "\n",
    "Game_date_Grizz = filtered_schedule_fin[((filtered_schedule_fin['away_team'] == 'MEM') | (filtered_schedule_fin['home_team'] == 'MEM'))]\n",
    "Game_date_Knicks = filtered_schedule_fin[((filtered_schedule_fin['away_team'] == 'NYK') | (filtered_schedule_fin['home_team'] == 'NYK'))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Gameday_Grizz = urls_df[urls_df['date'].isin(Game_date_Grizz['game_date']) & urls_df['URL'].str.contains('memphisgrizzlies')]\n",
    "Not_Gameday_Grizz = urls_df[~urls_df.index.isin(Gameday_Grizz.index) & urls_df['URL'].str.contains('memphisgrizzlies')]\n",
    "\n",
    "Gameday_Knicks = urls_df[urls_df['date'].isin(Game_date_Knicks['game_date']) & urls_df['URL'].str.contains('NYKnicks')]\n",
    "Not_Gameday_Knicks = urls_df[~urls_df.index.isin(Gameday_Knicks.index) & urls_df['URL'].str.contains('NYKnicks')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479dae5-ed64-4a1c-8cd4-fc52d40bf094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ee455-9dc2-43d2-ab86-6e4c3efcfaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_texts = []\n",
    "exception_urls = []\n",
    "\n",
    "for url in Gameday_Grizz['URL']:\n",
    "    reddit_text = \"\"\n",
    "    \n",
    "    try:\n",
    "        response = urlopen(url + '.json')\n",
    "        json_data = json.load(response)\n",
    "        \n",
    "        for item in json_data:\n",
    "            if 'data' in item and 'children' in item['data']:\n",
    "                for child in item['data']['children']:\n",
    "                    if 'data' in child and 'title' in child['data'] and 'selftext' in child['data']:\n",
    "                        reddit_text += child['data']['title'] + \"\\n\" + child['data']['selftext'] + \"\\n\"\n",
    "                    elif 'data' in child and 'body' in child['data']:\n",
    "                        reddit_text += child['data']['body'] + \"\\n\"\n",
    "        \n",
    "        link_texts.append(reddit_text)\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching data for URL:\", url, \"-\", e)\n",
    "        exception_urls.append(url)\n",
    "    \n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab7fd5-a2d9-46e1-a543-50ba138be1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_Gameday_Grizz = Gameday_Grizz[~Gameday_Grizz['URL'].isin(exception_urls)].copy()\n",
    "boolean_list = [True] * 224\n",
    "Text_Gameday_Grizz.loc[:, 'text'] = link_texts\n",
    "Text_Gameday_Grizz['game'] = boolean_list\n",
    "Text_Gameday_Grizz = Text_Gameday_Grizz.drop(columns = ['URL'])\n",
    "Text_Gameday_Grizz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d2a51-15c4-4e6b-bebf-da45a9f581b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_texts1 = []\n",
    "exception_urls1 = []\n",
    "\n",
    "for url in Not_Gameday_Grizz['URL']:\n",
    "    reddit_text = \"\"\n",
    "    \n",
    "    try:\n",
    "        response = urlopen(url + '.json')\n",
    "        json_data = json.load(response)\n",
    "        \n",
    "        for item in json_data:\n",
    "            if 'data' in item and 'children' in item['data']:\n",
    "                for child in item['data']['children']:\n",
    "                    if 'data' in child and 'title' in child['data'] and 'selftext' in child['data']:\n",
    "                        reddit_text += child['data']['title'] + \"\\n\" + child['data']['selftext'] + \"\\n\"\n",
    "                    elif 'data' in child and 'body' in child['data']:\n",
    "                        reddit_text += child['data']['body'] + \"\\n\"\n",
    "        \n",
    "        link_texts1.append(reddit_text)\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching data for URL:\", url, \"-\", e)\n",
    "        exception_urls1.append(url)\n",
    "    \n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d420f51b-cee6-4c95-93d5-eda6e273a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_Not_Gameday_Grizz = Not_Gameday_Grizz[~Not_Gameday_Grizz['URL'].isin(exception_urls1)].copy()\n",
    "boolean_list1 = [False] * 247\n",
    "Text_Not_Gameday_Grizz.loc[:, 'text'] = link_texts1\n",
    "Text_Not_Gameday_Grizz['game'] = boolean_list1\n",
    "Text_Not_Gameday_Grizz = Text_Not_Gameday_Grizz.drop(columns = ['URL'])\n",
    "Text_Not_Gameday_Grizz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d06f4-0fb0-48ab-865c-1d32d871a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Grizzlies_Text_DF = pd.concat([Text_Gameday_Grizz, Text_Not_Gameday_Grizz])\n",
    "Grizzlies_Text_DF = Grizzlies_Text_DF.sort_values(by='date')\n",
    "Grizzlies_Text_DF.reset_index(drop=True, inplace=True)\n",
    "Grizzlies_Text_DF#Exporting this DF to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1652b4-8e7a-4acd-b90e-0dd2bc4cae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_texts2 = []\n",
    "exception_urls2 = []\n",
    "\n",
    "for url in Gameday_Knicks['URL']:\n",
    "    reddit_text = \"\"\n",
    "    \n",
    "    try:\n",
    "        response = urlopen(url + '.json')\n",
    "        json_data = json.load(response)\n",
    "        \n",
    "        for item in json_data:\n",
    "            if 'data' in item and 'children' in item['data']:\n",
    "                for child in item['data']['children']:\n",
    "                    if 'data' in child and 'title' in child['data'] and 'selftext' in child['data']:\n",
    "                        reddit_text += child['data']['title'] + \"\\n\" + child['data']['selftext'] + \"\\n\"\n",
    "                    elif 'data' in child and 'body' in child['data']:\n",
    "                        reddit_text += child['data']['body'] + \"\\n\"\n",
    "        \n",
    "        link_texts2.append(reddit_text)\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching data for URL:\", url, \"-\", e)\n",
    "        exception_urls2.append(url)\n",
    "    \n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3417ce1d-e0eb-4b01-9612-16c95b4a3bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_Gameday_Knicks = Gameday_Knicks[~Gameday_Knicks['URL'].isin(exception_urls2)].copy()\n",
    "boolean_list2 = [True] * l\n",
    "Text_Gameday_Knicks.loc[:, 'text'] = link_texts2\n",
    "Text_Gameday_Knicks['game'] = boolean_list2\n",
    "Text_Gameday_Knicks = Text_Gameday_Knicks.drop(columns = ['URL'])\n",
    "Text_Gameday_Knicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a0bdf6-75de-43ee-87a9-ec3fcba04247",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_texts3 = []\n",
    "exception_urls3 = []\n",
    "\n",
    "for url in Not_Gameday_Knicks['URL']:\n",
    "    reddit_text = \"\"\n",
    "    \n",
    "    try:\n",
    "        response = urlopen(url + '.json')\n",
    "        json_data = json.load(response)\n",
    "        \n",
    "        for item in json_data:\n",
    "            if 'data' in item and 'children' in item['data']:\n",
    "                for child in item['data']['children']:\n",
    "                    if 'data' in child and 'title' in child['data'] and 'selftext' in child['data']:\n",
    "                        reddit_text += child['data']['title'] + \"\\n\" + child['data']['selftext'] + \"\\n\"\n",
    "                    elif 'data' in child and 'body' in child['data']:\n",
    "                        reddit_text += child['data']['body'] + \"\\n\"\n",
    "        \n",
    "        link_texts3.append(reddit_text)\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching data for URL:\", url, \"-\", e)\n",
    "        exception_urls3.append(url)\n",
    "    \n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020cdbd0-e18b-414c-b742-ee3cedf6be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_Not_Gameday_Knicks = Not_Gameday_Knicks[~Not_Gameday_Knicks['URL'].isin(exception_urls3)].copy()\n",
    "boolean_list3 = [False] * l1\n",
    "Text_Not_Gameday_Knicks.loc[:, 'text'] = link_texts3\n",
    "Text_Not_Gameday_Knicks['game'] = boolean_list3\n",
    "Text_Not_Gameday_Knicks = Text_Not_Gameday_Knicks.drop(columns = ['URL'])\n",
    "Text_Not_Gameday_Knicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28482be2-f6d5-473d-b71a-52e1b74de9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Knicks_Text_DF = pd.concat([Text_Gameday_Knicks, Text_Not_Gameday_Knicks])\n",
    "Knicks_Text_DF = Knicks_Text_DF.sort_values(by='date')\n",
    "Knicks_Text_DF.reset_index(drop=True, inplace=True)\n",
    "Knicks_Text_DF#Exporting this DF to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09516d1d-79b3-4f05-bbd9-f75928e4c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Knicks_Text_DF.to_csv('C:/Users/LukeB/OneDrive/Documents/Datasets/Knicks_Text_Data.csv', index = False)\n",
    "Grizzlies_Text_DF.to_csv('C:/Users/LukeB/OneDrive/Documents/Datasets/Grizzlies_Text_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de59922d-3ce7-4a6b-84dd-fb3add1e535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Grizzlies = pd.read_csv('C:/Users/LukeB/OneDrive/Documents/Datasets/Grizzlies_Text_data.csv')\n",
    "Knicks = pd.read_csv('C:/Users/LukeB/OneDrive/Documents/Datasets/Knicks_Text_Data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
